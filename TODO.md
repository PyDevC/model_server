# TODO

- Change the arch to allow faster loading for the server and faster loading in case of simpler actions
- Create complete server for local inferencing of model
- Create docs for hf_model
- Create a fake low memory model for testing, use fake tensors for inferencing
- Add more model cards for general purpose LLM
- write testcases for the memory of the model
- extend memory utilities and multithreading capabilities
- Add a logging system for server
- integrate hf_hub with downloading of the model
- Extend base_card  to get more information and capabilities for the model
- store conversation locally, and use the information for future inferencing
